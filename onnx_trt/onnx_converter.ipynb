{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we convert our pretrained fake-quantized pytorch models to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "import os\n",
    "import yaml\n",
    "from os.path import join, dirname\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pytorch_quantization.nn as quant_nn\n",
    "import ruamel.yaml\n",
    "from easydict import EasyDict as edict\n",
    "from quantization_libs.calibrator import collect_stats, compute_amax\n",
    "from utils.misc import setup_seed\n",
    "from dataset.build_dataset import get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and CLIP text embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#load model and quantization configs\n",
    "with open('../world_swin/configs/scannet/swint_mix_ctrs.yaml', 'r') as stream:\n",
    "    config = edict(ruamel.yaml.safe_load(stream))\n",
    "with open('../quantization_configs/jacob.yaml', 'r') as stream:\n",
    "    quant_config = edict(ruamel.yaml.safe_load(stream))\n",
    "config.update(quant_config)\n",
    "\n",
    "# load dataset\n",
    "dataset_val = get_dataset(config.DATA.DATASET)(\n",
    "            split='test',\n",
    "            data_dir=join('../dbs', config.DATA.DATASET),\n",
    "            depth_transform=config.DATA.DEPTH_TRANSFORM,\n",
    "            label_type='gt',\n",
    "        )\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=config.DATA.VAL_BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "\n",
    "print(f\"==> Dataset: {config.DATA.DATASET} Val set: {len(dataset_val)}, Batch size: {config.DATA.VAL_BATCH_SIZE}\")\n",
    "\n",
    "# load computed text_features to gpu 0 \n",
    "text_features = torch.load('text_features_scannet.pt', map_location='cpu')\n",
    "text_features = text_features.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    cnt_correct = 0\n",
    "    for batch_idx, (rgb_imgs, depth_imgs, class_id) in enumerate(tqdm(data_loader_val, leave=False)):\n",
    "        batch_size = rgb_imgs.shape[0]\n",
    "        batch_diff = config.DATA.VAL_BATCH_SIZE - batch_size\n",
    "\n",
    "        # padd tensors with 0s to make sure they have the same size\n",
    "        if rgb_imgs.shape[0] < config.DATA.VAL_BATCH_SIZE:\n",
    "            rgb_imgs = torch.cat([rgb_imgs, torch.zeros([batch_diff] + list(rgb_imgs.shape)[1:])], dim=0)\n",
    "            depth_imgs = torch.cat([depth_imgs, torch.zeros([batch_diff] + list(depth_imgs.shape)[1:])], dim=0)\n",
    "            class_id = torch.cat([class_id, torch.zeros([batch_diff] + list(class_id.shape)[1:])], dim=0)\n",
    "\n",
    "        if config.MODAL == 'rgb':\n",
    "            input_imgs = rgb_imgs.to(device)   # ([32, 4, 3, 224, 224]), ([32])\n",
    "        elif config.MODAL == 'depth':\n",
    "            input_imgs = depth_imgs.to(device) # ([32, 4, 3, 224, 224]), ([32])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model(input_imgs) # ([1, 512])\n",
    "\n",
    "        # Pick the top 5 most similar labels for the image\n",
    "        image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "        \n",
    "        similarity = (100.0 * image_features @ text_features.float().T).softmax(dim=-1) # ([1, 19])\n",
    "        for i in range(len(similarity)):\n",
    "            if i >= batch_size:\n",
    "                break\n",
    "            values, indices = similarity[i].topk(5)\n",
    "            if indices[0].item() == class_id[i].item():\n",
    "                cnt_correct += 1\n",
    "\n",
    "    acc1 = cnt_correct / len(data_loader_val.dataset)\n",
    "    return acc1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from world_swin.build_model import build_model\n",
    "from world_swin.build_model import load_weights\n",
    "from pytorch_quantization import quant_modules\n",
    "\n",
    "# load models\n",
    "model = build_model(config)\n",
    "load_weights(model, '../logs/swint_0218_143734/wandb/latest-run/files/src/best_model.pth')\n",
    "model.to(device)\n",
    "\n",
    "# fix the quantiztion scales as TensorRT only support static quantization\n",
    "with torch.no_grad():\n",
    "    collect_stats(model, data_loader_val, config.quantization, device)\n",
    "    compute_amax(model, config.quantization, device)\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pytorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'swin.onnx'\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model, dummy_input, model_path, verbose=False, opset_version=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
